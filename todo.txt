Remove any punctuation.
Split the paragraphs into the individual sentences, and append each to a list if there's more than one word.






Now that we have a corpus of text to work with, we need to get it into a format that we can process with a Tensorflow model. There are a number of steps to this process as well:

Tokenize the text, and create a dictionary of numeric word IDs and the corresponding words.
Create a list of sequences of word IDs that represents each of the sentences in the corpus, for each sub-sentence up to the full sentence.
Pre-pad the sequences with as many zeros as is required to ensure that all the sequences have the same length, i.e. the length of the longest sentence.
Break off the last word ID from each sequence, so we're left with the list of all the preceding sequences (the inputs) and a separate list of the final words (the result or label) from each sequence.
One-hot encode the list of labels.


Recurrent Neural Network (RNN) consisting of multiple bi-directional layers of Long Short Term Memory (LSTM) units.

The idea of an RNN is that it can handle "long term dependencies" by using the past information to help provide context to the present. This works well with relatively small gaps between the past and present, but not so well when the gaps become longer. This is where LSTM units help, as they are able to remember (and forget) data from much earlier in the sequence, enabling the network to better connect the past data with the present.

An embedding layer.
Two bidirectional LSTM layers.
A dropout layer.
A dense layer.

https://colah.github.io/posts/2015-08-Understanding-LSTMs/




The most widely used predictive models are:

Decision trees:
Decision trees are a simple, but powerful form of multiple variable analysis. They are produced by algorithms that identify various ways of splitting data into branch-like segments. Decision trees partition data into subsets based on categories of input variables, helping you to understand someone’s path of decisions.
Regression (linear and logistic)
Regression is one of the most popular methods in statistics. Regression analysis estimates relationships among variables, finding key patterns in large and diverse data sets and how they relate to each other.
Neural networks
Patterned after the operation of neuronsin the human brain, neural networks (also called artificial neural networks) are a variety of deep learning technologies. They’re typically used to solve complex pattern recognition problems – and are incredibly useful for analysing large data sets. They are great at handling nonlinear relationships in data – and work well when certain variables are unknown
Other classifiers:

Time Series Algorithms: Time series algorithms sequentially plot data and are useful for forecasting continuous values over time.
Clustering Algorithms: Clustering algorithms organise data into groups whose members are similar.
Outlier Detection Algorithms: Outlier detection algorithms focus on anomaly detection, identifying items, events or observations that do not conform to an expected pattern or standard within a data set.
Ensemble Models: Ensemble models use multiple machine learning algorithms to obtain better predictive performance than what could be obtained from one algorithm alone.
Factor Analysis: Factor analysis is a method used to describe variability and aims to find independent latent variables.
Naïve Bayes: The Naïve Bayes classifier allows us to predict a class/category based on a given set of features, using probability.
Support vector machines: Support vector machines are supervised machine learning techniques that use associated learning algorithms to analyse data and recognise patterns.


GPT, BERT

Līdzsvarotais korpuss

embertia

http://www.korpuss.lv/