Remove any punctuation.
Split the paragraphs into the individual sentences, and append each to a list if there's more than one word.






Now that we have a corpus of text to work with, we need to get it into a format that we can process with a Tensorflow model. There are a number of steps to this process as well:

Tokenize the text, and create a dictionary of numeric word IDs and the corresponding words.
Create a list of sequences of word IDs that represents each of the sentences in the corpus, for each sub-sentence up to the full sentence.
Pre-pad the sequences with as many zeros as is required to ensure that all the sequences have the same length, i.e. the length of the longest sentence.
Break off the last word ID from each sequence, so we're left with the list of all the preceding sequences (the inputs) and a separate list of the final words (the result or label) from each sequence.
One-hot encode the list of labels.


Recurrent Neural Network (RNN) consisting of multiple bi-directional layers of Long Short Term Memory (LSTM) units.

The idea of an RNN is that it can handle "long term dependencies" by using the past information to help provide context to the present. This works well with relatively small gaps between the past and present, but not so well when the gaps become longer. This is where LSTM units help, as they are able to remember (and forget) data from much earlier in the sequence, enabling the network to better connect the past data with the present.

An embedding layer.
Two bidirectional LSTM layers.
A dropout layer.
A dense layer.

https://colah.github.io/posts/2015-08-Understanding-LSTMs/